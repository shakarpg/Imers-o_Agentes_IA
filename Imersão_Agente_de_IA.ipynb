{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrY1iK9EN9K4wyH9O5Vfhp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shakarpg/Imers-o_Agentes_IA/blob/main/Imers%C3%A3o_Agente_de_IA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWuXABOe04lx",
        "outputId": "a6c019e2-3ebb-4318-8b96-439f938cd687"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade langchain langchain-google-genai google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importação da API"
      ],
      "metadata": {
        "id": "sNfC7JJ62zHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "MIANc3U_1xA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conexão com o Gemini"
      ],
      "metadata": {
        "id": "oyzfiBWe29iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0.0,\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "ToPcyOHi2rK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp_test = llm.invoke(\"me explique llm.invoke\")\n",
        "print(resp_test.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCBf7ukL3OVw",
        "outputId": "cc4a1021-210c-4593-beb0-32aa02fae27c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`llm.invoke()` es un método fundamental en frameworks como **LangChain** que te permite interactuar directamente con un **Modelo de Lenguaje Grande (LLM)** de una manera sencilla y estandarizada.\n",
            "\n",
            "Piensa en `llm.invoke()` como el botón \"Enviar\" directo a tu LLM.\n",
            "\n",
            "Aquí te lo explico en detalle:\n",
            "\n",
            "### ¿Qué es `llm.invoke()`?\n",
            "\n",
            "Es un método que se encuentra en los objetos que representan un LLM (ya sea un modelo de texto puro como `OpenAI` o un modelo de chat como `ChatOpenAI`, `ChatAnthropic`, etc.) dentro de un framework como LangChain.\n",
            "\n",
            "### ¿Qué hace?\n",
            "\n",
            "1.  **Toma una entrada (prompt):** Recibe tu pregunta, instrucción o contexto como un string o como un objeto `Message` (especialmente útil para modelos de chat).\n",
            "2.  **La envía al LLM:** Internamente, se encarga de formatear esa entrada y enviarla a la API del modelo de lenguaje que hayas configurado (por ejemplo, la API de OpenAI, Anthropic, Google, etc.).\n",
            "3.  **Espera la respuesta:** Espera a que el LLM procese tu entrada y genere una salida.\n",
            "4.  **Devuelve la salida:** Te entrega la respuesta del LLM.\n",
            "\n",
            "### ¿Por qué usar `llm.invoke()`? (Ventajas)\n",
            "\n",
            "*   **Simplicidad:** Es la forma más directa y limpia de obtener una respuesta de un LLM.\n",
            "*   **Estandarización:** Forma parte de la LangChain Expression Language (LCEL), lo que significa que es un componente básico que puedes encadenar fácilmente con otras operaciones (como parsers, plantillas de prompt, etc.).\n",
            "*   **Flexibilidad:** Puede aceptar diferentes tipos de entrada (un simple string, un `HumanMessage`, una lista de `Message`s para conversaciones).\n",
            "*   **Sincronía:** Por defecto, es una llamada síncrona, lo que significa que tu código esperará a que el LLM responda antes de continuar. (Existe `llm.ainvoke()` para operaciones asíncronas).\n",
            "*   **Moderno:** Es el método preferido y más moderno para interactuar con LLMs en LangChain, reemplazando o complementando métodos más antiguos como `llm.predict()` o `llm.generate()`.\n",
            "\n",
            "### ¿Cómo se usa? (Ejemplo práctico)\n",
            "\n",
            "Primero, necesitas instalar LangChain y tener una clave de API para el LLM que quieras usar (por ejemplo, OpenAI).\n",
            "\n",
            "```bash\n",
            "pip install langchain-openai\n",
            "```\n",
            "\n",
            "```python\n",
            "from langchain_openai import ChatOpenAI\n",
            "from langchain_core.messages import HumanMessage, SystemMessage\n",
            "\n",
            "# 1. Inicializa tu LLM\n",
            "# Asegúrate de tener tu clave de API de OpenAI configurada como variable de entorno\n",
            "# export OPENAI_API_KEY=\"tu_clave_aqui\"\n",
            "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7) # Puedes elegir el modelo que prefieras\n",
            "\n",
            "# --- Ejemplo 1: Con un prompt simple (string) ---\n",
            "print(\"--- Ejemplo 1: Prompt simple (string) ---\")\n",
            "prompt_simple = \"¿Cuál es la capital de Francia?\"\n",
            "respuesta_simple = llm.invoke(prompt_simple)\n",
            "\n",
            "print(f\"Prompt: {prompt_simple}\")\n",
            "print(f\"Tipo de respuesta: {type(respuesta_simple)}\")\n",
            "print(f\"Contenido de la respuesta: {respuesta_simple.content}\")\n",
            "# Salida esperada: La capital de Francia es París.\n",
            "\n",
            "\n",
            "# --- Ejemplo 2: Con un objeto HumanMessage (más común para modelos de chat) ---\n",
            "print(\"\\n--- Ejemplo 2: Con HumanMessage ---\")\n",
            "prompt_message = HumanMessage(content=\"Explícame brevemente la teoría de la relatividad.\")\n",
            "respuesta_message = llm.invoke(prompt_message)\n",
            "\n",
            "print(f\"Prompt: {prompt_message.content}\")\n",
            "print(f\"Tipo de respuesta: {type(respuesta_message)}\")\n",
            "print(f\"Contenido de la respuesta: {respuesta_message.content}\")\n",
            "# Salida esperada: La teoría de la relatividad de Einstein...\n",
            "\n",
            "\n",
            "# --- Ejemplo 3: Con una lista de mensajes (para mantener el contexto de una conversación) ---\n",
            "print(\"\\n--- Ejemplo 3: Con lista de mensajes (conversación) ---\")\n",
            "mensajes_conversacion = [\n",
            "    SystemMessage(content=\"Eres un asistente útil y conciso.\"),\n",
            "    HumanMessage(content=\"¿Quién fue el primer presidente de Estados Unidos?\"),\n",
            "]\n",
            "respuesta_conversacion = llm.invoke(mensajes_conversacion)\n",
            "\n",
            "print(f\"Mensajes enviados: {mensajes_conversacion}\")\n",
            "print(f\"Tipo de respuesta: {type(respuesta_conversacion)}\")\n",
            "print(f\"Contenido de la respuesta: {respuesta_conversacion.content}\")\n",
            "# Salida esperada: George Washington.\n",
            "\n",
            "# Puedes añadir más mensajes para continuar la conversación\n",
            "mensajes_conversacion.append(respuesta_conversacion) # Añade la respuesta del LLM\n",
            "mensajes_conversacion.append(HumanMessage(content=\"¿Y cuál fue su principal logro?\"))\n",
            "respuesta_conversacion_2 = llm.invoke(mensajes_conversacion)\n",
            "\n",
            "print(f\"\\nSegundo prompt en la conversación: {mensajes_conversacion[-1].content}\")\n",
            "print(f\"Contenido de la segunda respuesta: {respuesta_conversacion_2.content}\")\n",
            "# Salida esperada: Su principal logro fue liderar al país durante la Guerra de Independencia...\n",
            "```\n",
            "\n",
            "### Parámetros clave de `llm.invoke()`\n",
            "\n",
            "*   **`input` (obligatorio):** El prompt o los mensajes que quieres enviar al LLM. Puede ser un `str`, un `BaseMessage` (como `HumanMessage`, `AIMessage`, `SystemMessage`), o una lista de `BaseMessage`s.\n",
            "*   **`config` (opcional):** Un diccionario para configurar aspectos como callbacks, tags, etc., que son útiles para monitorear y depurar tus cadenas.\n",
            "*   **`stop` (opcional):** Una lista de strings que, si el LLM los genera, harán que detenga la generación de texto inmediatamente.\n",
            "\n",
            "### Valor de retorno\n",
            "\n",
            "`llm.invoke()` generalmente devuelve un objeto `BaseMessage` (como `AIMessage` para modelos de chat) o un `str` (para modelos de texto más antiguos). En el caso de `ChatOpenAI`, siempre devuelve un `AIMessage` que tiene un atributo `.content` donde se encuentra la respuesta en texto.\n",
            "\n",
            "### En resumen\n",
            "\n",
            "`llm.invoke()` es la forma más directa, moderna y recomendada de enviar una entrada a un LLM y obtener su respuesta en LangChain. Es un bloque de construcción fundamental para crear aplicaciones más complejas con LLMs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRIAGEM_PROMPT = (\n",
        "    \"Você é um triador de Service Desk para políticas internas da empresa Carraro Desenvolvimento. \"\n",
        "    \"Dada a mensagem do usuário, retorne SOMENTE um JSON com:\\n\"\n",
        "    \"{\\n\"\n",
        "    '  \"decisao\": \"AUTO_RESOLVER\" | \"PEDIR_INFO\" | \"ABRIR_CHAMADO\",\\n'\n",
        "    '  \"urgencia\": \"BAIXA\" | \"MEDIA\" | \"ALTA\",\\n'\n",
        "    '  \"campos_faltantes\": [\"...\"]\\n'\n",
        "    \"}\\n\"\n",
        "    \"Regras:\\n\"\n",
        "    '- **AUTO_RESOLVER**: Perguntas claras sobre regras ou procedimentos descritos nas políticas (Ex: \"Posso reembolsar a internet do meu home office?\", \"Como funciona a política de alimentação em viagens?\").\\n'\n",
        "    '- **PEDIR_INFO**: Mensagens vagas ou que faltam informações para identificar o tema ou contexto (Ex: \"Preciso de ajuda com uma política\", \"Tenho uma dúvida geral\").\\n'\n",
        "    '- **ABRIR_CHAMADO**: Pedidos de exceção, liberação, aprovação ou acesso especial, ou quando o usuário explicitamente pede para abrir um chamado (Ex: \"Quero exceção para trabalhar 5 dias remoto.\", \"Solicito liberação para anexos externos.\", \"Por favor, abra um chamado para o RH.\").'\n",
        "    \"Analise a mensagem e decida a ação mais apropriada.\"\n",
        ")"
      ],
      "metadata": {
        "id": "oeP9dtjB3lIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal, List, Dict\n",
        "\n",
        "class TriagemOut(BaseModel):\n",
        "    decisao: Literal[\"AUTO_RESOLVER\", \"PEDIR_INFO\", \"ABRIR_CHAMADO\"]\n",
        "    urgencia: Literal[\"BAIXA\", \"MEDIA\", \"ALTA\"]\n",
        "    campos_faltantes: List[str] = Field(default_factory=list)"
      ],
      "metadata": {
        "id": "DaJhuk7h31HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_triagem = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0.0,\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "uMZWSwh_328j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "triagem_chain = llm_triagem.with_structured_output(TriagemOut)\n",
        "\n",
        "def triagem(mensagem: str) -> Dict:\n",
        "    saida: TriagemOut = triagem_chain.invoke([\n",
        "        SystemMessage(content=TRIAGEM_PROMPT),\n",
        "        HumanMessage(content=mensagem)\n",
        "    ])\n",
        "\n",
        "    return saida.model_dump()"
      ],
      "metadata": {
        "id": "Xrv8UyZG36SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros?\"]"
      ],
      "metadata": {
        "id": "ptZcjxVl4Ayp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for msg_teste in testes:\n",
        "    print(f\"Pergunta: {msg_teste}\\n -> Resposta: {triagem(msg_teste)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HvADUc94De5",
        "outputId": "1ac16f8b-88c9-4c21-e920-23a69bc3de5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pergunta: Posso reembolsar a internet?\n",
            " -> Resposta: {'decisao': 'AUTO_RESOLVER', 'urgencia': 'BAIXA', 'campos_faltantes': []}\n",
            "\n",
            "Pergunta: Quero mais 5 dias de trabalho remoto. Como faço?\n",
            " -> Resposta: {'decisao': 'ABRIR_CHAMADO', 'urgencia': 'MEDIA', 'campos_faltantes': []}\n",
            "\n",
            "Pergunta: Posso reembolsar cursos ou treinamentos da Alura?\n",
            " -> Resposta: {'decisao': 'AUTO_RESOLVER', 'urgencia': 'BAIXA', 'campos_faltantes': []}\n",
            "\n",
            "Pergunta: Quantas capivaras tem no Rio Pinheiros?\n",
            " -> Resposta: {'decisao': 'PEDIR_INFO', 'urgencia': 'BAIXA', 'campos_faltantes': ['informação sobre a política interna']}\n",
            "\n"
          ]
        }
      ]
    }
  ]
}